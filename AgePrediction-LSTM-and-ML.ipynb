{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2c0_0uImBwm5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# For visualize input\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    '''\n",
    "    Custom Dataset object for PyTorch to load the dataset\n",
    "    '''\n",
    "    def __init__(self, x, y, train, val):\n",
    "        super(EEGDataset).__init__()\n",
    "        assert x.shape[0] == y.size\n",
    "        self.x = x\n",
    "        self.y = [y[i][0] for i in range(y.size)]\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "\n",
    "    def __getitem__(self,key):\n",
    "        return (self.x[key], self.y[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "class Logger():\n",
    "    '''\n",
    "    Object controlling how information will be logged\n",
    "    A logger created globally will be used to log all information\n",
    "    Create a Logger(mode='debug') to have everything print to the console\n",
    "    '''\n",
    "    def __init__(self, mode='log'):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def set_model_save_location(self, model_dir):\n",
    "        self.model_dir = f\"saved-model/{model_dir}\"\n",
    "        if not os.path.isdir(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "        \n",
    "    def set_experiment(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        log_format = '%(asctime)s %(message)s'\n",
    "        logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                            format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "        fh = logging.FileHandler(os.path.join('training-logs', f'log-{experiment_name}-{datetime.datetime.today()}.txt'))\n",
    "        fh.setFormatter(logging.Formatter(log_format))\n",
    "        logging.getLogger().addHandler(fh)\n",
    "        self.writer = SummaryWriter(f\"runs/{experiment_name}\")\n",
    "            \n",
    "    def log(self, message=\"\"):\n",
    "        if self.mode == 'log':\n",
    "            logging.info(message)\n",
    "        elif self.mode == 'debug':\n",
    "            print(message)\n",
    "\n",
    "    def save_model(self, model, info):\n",
    "        torch.save(model.state_dict(), f\"{self.model_dir}/model-{logger.experiment_name}-{info}\")\n",
    "        \n",
    "def load_data(path, role, winLength, numChan, srate, feature, one_channel=False, version=\"\"):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    :param  \n",
    "        path: Filepath to the dataset\n",
    "        role: Role of the dataset. Can be \"train\", \"val\", or \"test\"\n",
    "        winLength: Length of time window. Can be 2 or 15\n",
    "        numChan: Number of channels. Can be 24 or 128\n",
    "        srate: Sampling rate. Supporting 126Hz\n",
    "        feature: Input feature. Can be \"raw\", \"spectral\", or \"topo\"\n",
    "        one_channel: Whether input has 1 or 3 channel in depth dimension. Matters when load topo data as number of input channels \n",
    "                are different from original's\n",
    "        version: Any additional information of the datafile. Will be appended to the file name at the end\n",
    "    \"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    x = f[f'X_{role}']\n",
    "    if feature == 'raw':\n",
    "        x = np.transpose(x,(0,2,1))\n",
    "        x = np.reshape(x,(-1,1,numChan,winLength*srate))\n",
    "    elif feature == 'topo':\n",
    "        if one_channel:\n",
    "            samples = []\n",
    "            for i in range(x.shape[0]):\n",
    "                image = x[i]\n",
    "                b, g, r = image[0,:, :], image[1,:, :], image[2,:, :]\n",
    "                concat = np.concatenate((b,g,r), axis=1)\n",
    "                samples.append(concat)\n",
    "            x = np.stack(samples)\n",
    "            x = np.reshape(x,(-1,1,x.shape[1],x.shape[2]))\n",
    "    \n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_yclass1_ages_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_yclass1_ages_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    y = np.subtract(f[f'Y_cls_{role}'], 1)\n",
    "   \n",
    "    return x,y\n",
    "\n",
    "\n",
    "\n",
    "def plot_to_image_tensor(figure):\n",
    "    # Save the plot to a PNG in memory.\n",
    "    figure.savefig('batch.png')\n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    img = Image.open('batch.png')\n",
    "    trans = transforms.ToPILImage()\n",
    "    trans1 = transforms.ToTensor()\n",
    "    image_tensor = trans1(img)\n",
    "    return image_tensor\n",
    "\n",
    "def plot_EEG(data, feature, numChan, one_channel=True):\n",
    "    '''\n",
    "    Plot EEG sample\n",
    "    :param\n",
    "        data: An EEGDataset object\n",
    "        feature: String - 'raw' or 'topo'\n",
    "        numChan: Int - number of EEG channels\n",
    "        one_channel: Bool - Whether input has 1 or 3 channel in depth dimension. Matters when load topo data as number of input channels \n",
    "                are different from original's\n",
    "    '''\n",
    "    x_data = data[:][0]\n",
    "    if feature == 'raw':        \n",
    "        fig = plt.figure(figsize=(80, 80))\n",
    "        outer = gridspec.GridSpec(8, 8)\n",
    "        for i in range(64):\n",
    "            inner = gridspec.GridSpecFromSubplotSpec(numChan, 1,\n",
    "                            subplot_spec=outer[i])\n",
    "#             npimg = img[i,:,:,:].numpy()\n",
    "            npimg = x_data[i,:,:,:]\n",
    "            npimg = np.reshape(npimg,(24,256))\n",
    "            yax = None\n",
    "            for j in range(24):\n",
    "                ax = plt.Subplot(fig, inner[j])\n",
    "                ax.plot(range(256),npimg[j,:],'k')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                fig.add_subplot(ax)\n",
    "\n",
    "        return fig\n",
    "    else:\n",
    "        sample = 2\n",
    "        if one_channel:\n",
    "            image = np.reshape(x_data[sample], (x_data[sample].shape[1],x_data[sample].shape[2]))\n",
    "            plt.imshow(image.astype('int32'))\n",
    "        else:\n",
    "            plt.imshow(np.transpose(x_data[sample].astype('int32'), (1, 2, 0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EEG data\n",
    "path = './data/'\n",
    "winLength = 2\n",
    "numChan = 24\n",
    "srate = 128\n",
    "feature = 'raw'\n",
    "one_channel = False\n",
    "\n",
    "role = 'train'\n",
    "train_data_x, train_data_labels = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n",
    "# print(f'X_train shape: {len(train_data)}, {train_data[0][0].shape}')\n",
    "# print(f'Y_train shape: {len(train_data)}, {train_data[0][1].shape}')\n",
    "\n",
    "role = 'val'\n",
    "val_data_x, val_data_labels = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n",
    "# print(f'X_val shape: {len(val_data)}, {val_data[0][0].shape}')\n",
    "# print(f'Y_val shape: {len(val_data)}, {val_data[0][1].shape}')\n",
    "# EEGDataset(x, y, role=='train', role=='val')\n",
    "# plot_EEG(train_data, feature, numChan, one_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_data_labels.copy()\n",
    "train_data_y[np.logical_and(train_data_labels <= 2,train_data_labels >= 0)] = 0\n",
    "train_data_y[train_data_labels > 2] = 1\n",
    "val_data_y = val_data_labels.copy()\n",
    "val_data_y[np.logical_and(val_data_labels <= 2,val_data_labels >= 0)] = 0\n",
    "val_data_y[val_data_labels > 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([43597.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0., 27784.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.610764769336378"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43597/(43597+27784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24980.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0., 14888.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(val_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6265676733219625"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24980/(24980+14888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EEGDataset(train_data_x, train_data_y, True, False)\n",
    "val_data = EEGDataset(val_data_x, val_data_y, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x = train_data_x.reshape((71381, 24, 256))\n",
    "val_data_x = val_data_x.reshape((39868, 24, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EEGDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-90df03dce17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'EEGDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_data.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version=1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"tf version={tf.__version__}\")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 24, 64)            49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 12, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 6, 128)            24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 3, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 128)            512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 563,714\n",
      "Trainable params: 563,330\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(24, 256)))\n",
    "model.add(MaxPooling1D(2, 2, padding='same'))\n",
    "model.add((Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(MaxPooling1D(2, 2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add((Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(MaxPooling1D(2, 2, padding='same'))\n",
    "model.add((Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(MaxPooling1D(2, 2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add((Dropout(0.3)))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71381 samples, validate on 39868 samples\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "71381/71381 [==============================] - 43s 605us/sample - loss: 9.8387 - acc: 0.3893 - val_loss: 10.0991 - val_acc: 0.3734\n",
      "Epoch 2/50\n",
      "71381/71381 [==============================] - 42s 588us/sample - loss: 9.8444 - acc: 0.3892 - val_loss: 10.0991 - val_acc: 0.3734\n",
      "Epoch 3/50\n",
      "28480/71381 [==========>...................] - ETA: 21s - loss: 9.8780 - acc: 0.3871"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-274c2d93d808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_data_x, train_data_y, batch_size=64, epochs=50, validation_data=(val_data_x, val_data_y), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(val_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = [p[0]<p[1] for p in val_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71381, 24, 256, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x = np.swapaxes(train_data_x, 1, 3)\n",
    "train_data_x = np.swapaxes(train_data_x, 1, 2)\n",
    "train_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_x = np.swapaxes(val_data_x, 1, 3)\n",
    "val_data_x = np.swapaxes(val_data_x, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = tf.keras.utils.to_categorical(train_data_y)\n",
    "val_data_y = tf.keras.utils.to_categorical(val_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 22, 254, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 11, 127, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 127, 100)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 125, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 62, 100)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 62, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 60, 300)        180300    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 30, 300)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 30, 300)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 24, 300)        630300    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 23, 300)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 23, 300)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 21, 100)        90100     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 19, 100)        30100     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1900)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6144)              11679744  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 12290     \n",
      "=================================================================\n",
      "Total params: 12,713,934\n",
      "Trainable params: 12,713,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(100, 3, activation='relu', input_shape=(24, 256, 1)))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(100, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(300, (2, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(300, (1, 7), activation='relu'))\n",
    "model.add(MaxPooling2D((1, 2), 1))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(100, (1, 3), activation='relu'))\n",
    "model.add(Conv2D(100, (1, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6144, activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"relu\"))\n",
    "\n",
    "adam = keras.optimizers.Adam(learning_rate=0.002)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-model-summary in /home/yiy287/.local/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch-model-summary) (4.32.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch-model-summary) (1.16.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pytorch-model-summary) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-model-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and show model summary\n",
    "model = create_model()\n",
    "#from pytorch_model_summary import summary\n",
    "#print(summary(model, torch.zeros((1, 1, 24, 256)), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x, test_data_labels = load_data(path, 'test', winLength, numChan, srate, feature, one_channel)\n",
    "test_data_y = test_data_labels.copy()\n",
    "test_data_y[np.logical_and(test_data_labels <= 2,test_data_labels >= 0)] = 0\n",
    "test_data_y[test_data_labels > 2] = 1\n",
    "test_data = EEGDataset(test_data_x, test_data_y, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved-model/original-ages-classification-raw/model-original-ages-classification-raw-seed0-valacc70-epoch49'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 11501 / 15925 correct (72.22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7221978021978022"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(mode='debug')\n",
    "loader_test = DataLoader(test_data, batch_size=1)\n",
    "model.to(device=device)\n",
    "check_accuracy(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8797.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        7128.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOy0lEQVR4nO3ccaydd13H8ffHls0BTjp3t8zbYYupQLdIYHVWUILMZB0YOxOWVIU1ZEnjnIjGRDr+kD9Mk5EYg4tupBm4LhCaZiyuikOXIqJhbN7BoHR1rjLs6up6QYWJybDj6x/n98exve19ut57Lre/9ys5Oc/5nec55/dLl/d99tx7TqoKSVIffmCpJyBJmhyjL0kdMfqS1BGjL0kdMfqS1JGVSz2B+Vx88cW1Zs2apZ6GJC0rjz766DeqaurE8e/76K9Zs4aZmZmlnoYkLStJ/nWucS/vSFJHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHvu8/kXs21mz/1JK879dve/uSvK8kzcczfUnqiNGXpI4YfUnqiNGXpI4YfUnqiNGXpI4YfUnqiNGXpI4YfUnqiNGXpI4YfUnqiNGXpI4YfUnqiNGXpI4Min6S30lyIMlXk3wiyQ8muSjJg0mebPerxva/NcmhJE8kuXZs/Kok+9tztyfJYixKkjS3eaOfZBr4LWBDVV0JrAC2ANuBfVW1DtjXHpNkfXv+CmATcEeSFe3l7gS2AevabdOCrkaSdFpDL++sBC5IshJ4KfAMsBnY1Z7fBVzftjcDu6vq+ap6CjgEXJ3kMuDCqnqoqgq4Z+wYSdIEzBv9qvo34A+Bw8BR4FtV9TfApVV1tO1zFLikHTINPD32Ekfa2HTbPnH8JEm2JZlJMjM7O3tmK5IkndKQyzurGJ29rwV+FHhZknee7pA5xuo04ycPVu2sqg1VtWFqamq+KUqSBhpyeecXgKeqaraq/he4D3gj8Gy7ZEO7P9b2PwJcPnb8akaXg4607RPHJUkTMiT6h4GNSV7a/trmGuAgsBfY2vbZCtzftvcCW5Kcn2Qto1/YPtIuAT2XZGN7nRvHjpEkTcDK+XaoqoeT3At8ETgOfAnYCbwc2JPkJkY/GG5o+x9Isgd4vO1/S1W90F7uZuBu4ALggXaTJE3IvNEHqKoPAB84Yfh5Rmf9c+2/A9gxx/gMcOUZzlGStED8RK4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdWTQ9+lLUq/WbP/Ukrzv1297+6K8rmf6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHTH6ktQRoy9JHRkU/SSvSHJvkn9KcjDJzyS5KMmDSZ5s96vG9r81yaEkTyS5dmz8qiT723O3J8liLEqSNLehZ/p/DHy6ql4DvA44CGwH9lXVOmBfe0yS9cAW4ApgE3BHkhXtde4EtgHr2m3TAq1DkjTAvNFPciHwZuAjAFX13ar6L2AzsKvttgu4vm1vBnZX1fNV9RRwCLg6yWXAhVX1UFUVcM/YMZKkCRhypv8qYBb4syRfSnJXkpcBl1bVUYB2f0nbfxp4euz4I21sum2fOC5JmpAh0V8JvAG4s6peD3yHdinnFOa6Tl+nGT/5BZJtSWaSzMzOzg6YoiRpiCHRPwIcqaqH2+N7Gf0QeLZdsqHdHxvb//Kx41cDz7Tx1XOMn6SqdlbVhqraMDU1NXQtkqR5zBv9qvp34Okkr25D1wCPA3uBrW1sK3B/294LbElyfpK1jH5h+0i7BPRcko3tr3ZuHDtGkjQBKwfu9x7g40nOA74GvJvRD4w9SW4CDgM3AFTVgSR7GP1gOA7cUlUvtNe5GbgbuAB4oN0kSRMyKPpV9RiwYY6nrjnF/juAHXOMzwBXnsH8JEkLyE/kSlJHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdcToS1JHjL4kdWRw9JOsSPKlJH/ZHl+U5MEkT7b7VWP73prkUJInklw7Nn5Vkv3tuduTZGGXI0k6nTM5038vcHDs8XZgX1WtA/a1xyRZD2wBrgA2AXckWdGOuRPYBqxrt01nNXtJ0hkZFP0kq4G3A3eNDW8GdrXtXcD1Y+O7q+r5qnoKOARcneQy4MKqeqiqCrhn7BhJ0gQMPdP/EPB7wPfGxi6tqqMA7f6SNj4NPD2235E2Nt22TxyXJE3IvNFP8ovAsap6dOBrznWdvk4zPtd7bksyk2RmdnZ24NtKkuYz5Ez/TcAvJfk6sBt4a5KPAc+2Sza0+2Nt/yPA5WPHrwaeaeOr5xg/SVXtrKoNVbVhamrqDJYjSTqdeaNfVbdW1eqqWsPoF7Sfqap3AnuBrW23rcD9bXsvsCXJ+UnWMvqF7SPtEtBzSTa2v9q5cewYSdIErDyLY28D9iS5CTgM3ABQVQeS7AEeB44Dt1TVC+2Ym4G7gQuAB9pNkjQhZxT9qvos8Nm2/U3gmlPstwPYMcf4DHDlmU5SkrQw/ESuJHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHXE6EtSR4y+JHVk3ugnuTzJ3yY5mORAkve28YuSPJjkyXa/auyYW5McSvJEkmvHxq9Ksr89d3uSLM6yJElzGXKmfxz43ap6LbARuCXJemA7sK+q1gH72mPac1uAK4BNwB1JVrTXuhPYBqxrt00LuBZJ0jzmjX5VHa2qL7bt54CDwDSwGdjVdtsFXN+2NwO7q+r5qnoKOARcneQy4MKqeqiqCrhn7BhJ0gSc0TX9JGuA1wMPA5dW1VEY/WAALmm7TQNPjx12pI1Nt+0Tx+d6n21JZpLMzM7OnskUJUmnMTj6SV4OfBL47ar69ul2nWOsTjN+8mDVzqraUFUbpqamhk5RkjSPQdFP8hJGwf94Vd3Xhp9tl2xo98fa+BHg8rHDVwPPtPHVc4xLkiZkyF/vBPgIcLCq/mjsqb3A1ra9Fbh/bHxLkvOTrGX0C9tH2iWg55JsbK9549gxkqQJWDlgnzcB7wL2J3msjb0fuA3Yk+Qm4DBwA0BVHUiyB3ic0V/+3FJVL7TjbgbuBi4AHmg3SdKEzBv9qvoH5r4eD3DNKY7ZAeyYY3wGuPJMJihJWjh+IleSOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjRl+SOmL0JakjE49+kk1JnkhyKMn2Sb+/JPVsotFPsgL4U+A6YD3wK0nWT3IOktSzSZ/pXw0cqqqvVdV3gd3A5gnPQZK6tXLC7zcNPD32+Ajw0yfulGQbsK09/O8kT7zI97sY+MaLPPZFywcn/Y7/z5KseYm55nNfb+slHzzrNf/YXIOTjn7mGKuTBqp2AjvP+s2SmaracLavs5y45j70tube1guLt+ZJX945Alw+9ng18MyE5yBJ3Zp09P8RWJdkbZLzgC3A3gnPQZK6NdHLO1V1PMlvAn8NrAA+WlUHFvEtz/oS0TLkmvvQ25p7Wy8s0ppTddIldUnSOcpP5EpSR4y+JHXknIj+fF/tkJHb2/NfSfKGpZjnQhmw3l9r6/xKks8ned1SzHMhDf36jiQ/leSFJO+Y5PwWw5A1J3lLkseSHEjyd5Oe40Ib8N/2Dyf5iyRfbmt+91LMc6Ek+WiSY0m+eornF75dVbWsb4x+IfwvwKuA84AvA+tP2OdtwAOMPiewEXh4qee9yOt9I7CqbV+3nNc7dM1j+30G+CvgHUs97wn8O78CeBx4ZXt8yVLPewJrfj/wwbY9BfwHcN5Sz/0s1vxm4A3AV0/x/IK361w40x/y1Q6bgXtq5AvAK5JcNumJLpB511tVn6+q/2wPv8Do8xDL2dCv73gP8Eng2CQnt0iGrPlXgfuq6jBAVS33dQ9ZcwE/lCTAyxlF//hkp7lwqupzjNZwKgvernMh+nN9tcP0i9hnuTjTtdzE6ExhOZt3zUmmgV8GPjzBeS2mIf/OPwGsSvLZJI8muXFis1scQ9b8J8BrGX2ocz/w3qr63mSmtyQWvF2T/hqGxTDkqx0Gff3DMjF4LUl+nlH0f3ZRZ7T4hqz5Q8D7quqF0UngsjdkzSuBq4BrgAuAh5J8oar+ebEnt0iGrPla4DHgrcCPAw8m+fuq+vYiz22pLHi7zoXoD/lqh3Pp6x8GrSXJTwJ3AddV1TcnNLfFMmTNG4DdLfgXA29Lcryq/nwiM1x4Q/+7/kZVfQf4TpLPAa8Dlmv0h6z53cBtNbrgfSjJU8BrgEcmM8WJW/B2nQuXd4Z8tcNe4Mb2m/CNwLeq6uikJ7pA5l1vklcC9wHvWsZnfePmXXNVra2qNVW1BrgX+I1lHHwY9t/1/cDPJVmZ5KWMvrH24ITnuZCGrPkwo/+zIcmlwKuBr010lpO14O1a9mf6dYqvdkjy6+35DzP6a463AYeA/2F0trAsDVzv7wM/AtzRznyP1zL+hsKBaz6nDFlzVR1M8mngK8D3gLuqas4//VsOBv47/wFwd5L9jC59vK+qlu1XLif5BPAW4OIkR4APAC+BxWuXX8MgSR05Fy7vSJIGMvqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kd+T94qfn7aghQPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5524018838304553"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8797/(8797+7128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x = train_data_x.reshape((71381, 24*256))\n",
    "train_y = train_data_y.reshape((71381))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32min 5s, sys: 7.45 s, total: 32min 12s\n",
      "Wall time: 32min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=500, random_state=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "n_train = 5000\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(train_x[:n_train], train_y[:n_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.9686\n",
      "val acc = 0.6268435838266279\n",
      "test acc = 0.5940973312401884\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf.predict(train_x[:n_train])\n",
    "accu_train = sum(train_pred == train_y[:n_train]) / n_train\n",
    "print(f'train acc = {accu_train}')\n",
    "\n",
    "val_pred = clf.predict(val_data_x.reshape((39868, 24*256)))\n",
    "val_y = val_data_y.reshape((39868))\n",
    "accu_val = sum(val_pred == val_y) / len(val_y)\n",
    "print(f'val acc = {accu_val}')\n",
    "\n",
    "test_pred = clf.predict(test_data_x.reshape((15925, 24*256)))\n",
    "test_y = test_data_y.reshape((15925))\n",
    "accu_val = sum(test_pred == test_y) / len(test_y)\n",
    "print(f'test acc = {accu_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.389235230663622\n",
      "0.3734323266780375\n",
      "0.44759811616954476\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_y)/len(train_y))\n",
    "print(sum(val_y)/len(val_y))\n",
    "print(sum(test_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.2 s, sys: 139 ms, total: 38.3 s\n",
      "Wall time: 38.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "n_train = 10000\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(train_x[:n_train], train_y[:n_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.6691\n",
      "val acc = 0.6297782682853417\n",
      "test acc = 0.6087912087912087\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf.predict(train_x[:n_train])\n",
    "accu_train = sum(train_pred == train_y[:n_train]) / n_train\n",
    "print(f'train acc = {accu_train}')\n",
    "\n",
    "val_pred = clf.predict(val_data_x.reshape((39868, 24*256)))\n",
    "val_y = val_data_y.reshape((39868))\n",
    "accu_val = sum(val_pred == val_y) / len(val_y)\n",
    "print(f'val acc = {accu_val}')\n",
    "\n",
    "test_pred = clf.predict(test_data_x.reshape((15925, 24*256)))\n",
    "test_y = test_data_y.reshape((15925))\n",
    "accu_val = sum(test_pred == test_y) / len(test_y)\n",
    "print(f'test acc = {accu_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 2.98 s, total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=5, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "n_train = 10000\n",
    "clf = RandomForestClassifier(n_estimators=200, min_samples_split=10, min_samples_leaf=5, random_state=0)\n",
    "clf.fit(train_x[:n_train], train_y[:n_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.9903\n",
      "val acc = 0.6881709641817999\n",
      "test acc = 0.6462166405023548\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf.predict(train_x[:n_train])\n",
    "accu_train = sum(train_pred == train_y[:n_train]) / n_train\n",
    "print(f'train acc = {accu_train}')\n",
    "\n",
    "val_pred = clf.predict(val_data_x.reshape((39868, 24*256)))\n",
    "val_y = val_data_y.reshape((39868))\n",
    "accu_val = sum(val_pred == val_y) / len(val_y)\n",
    "print(f'val acc = {accu_val}')\n",
    "\n",
    "test_pred = clf.predict(test_data_x.reshape((15925, 24*256)))\n",
    "test_y = test_data_y.reshape((15925))\n",
    "accu_val = sum(test_pred == test_y) / len(test_y)\n",
    "print(f'test acc = {accu_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SexPrediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
