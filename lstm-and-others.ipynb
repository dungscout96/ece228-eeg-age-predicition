{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2c0_0uImBwm5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# For visualize input\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    '''\n",
    "    Custom Dataset object for PyTorch to load the dataset\n",
    "    '''\n",
    "    def __init__(self, x, y, train, val):\n",
    "        super(EEGDataset).__init__()\n",
    "        assert x.shape[0] == y.size\n",
    "        self.x = x\n",
    "        self.y = [y[i][0] for i in range(y.size)]\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "\n",
    "    def __getitem__(self,key):\n",
    "        return (self.x[key], self.y[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "class Logger():\n",
    "    '''\n",
    "    Object controlling how information will be logged\n",
    "    A logger created globally will be used to log all information\n",
    "    Create a Logger(mode='debug') to have everything print to the console\n",
    "    '''\n",
    "    def __init__(self, mode='log'):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def set_model_save_location(self, model_dir):\n",
    "        self.model_dir = f\"saved-model/{model_dir}\"\n",
    "        if not os.path.isdir(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "        \n",
    "    def set_experiment(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        log_format = '%(asctime)s %(message)s'\n",
    "        logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                            format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "        fh = logging.FileHandler(os.path.join('training-logs', f'log-{experiment_name}-{datetime.datetime.today()}.txt'))\n",
    "        fh.setFormatter(logging.Formatter(log_format))\n",
    "        logging.getLogger().addHandler(fh)\n",
    "        self.writer = SummaryWriter(f\"runs/{experiment_name}\")\n",
    "            \n",
    "    def log(self, message=\"\"):\n",
    "        if self.mode == 'log':\n",
    "            logging.info(message)\n",
    "        elif self.mode == 'debug':\n",
    "            print(message)\n",
    "\n",
    "    def save_model(self, model, info):\n",
    "        torch.save(model.state_dict(), f\"{self.model_dir}/model-{logger.experiment_name}-{info}\")\n",
    "        \n",
    "def load_data(path, role, winLength, numChan, srate, feature, one_channel=False, version=\"\"):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    :param  \n",
    "        path: Filepath to the dataset\n",
    "        role: Role of the dataset. Can be \"train\", \"val\", or \"test\"\n",
    "        winLength: Length of time window. Can be 2 or 15\n",
    "        numChan: Number of channels. Can be 24 or 128\n",
    "        srate: Sampling rate. Supporting 126Hz\n",
    "        feature: Input feature. Can be \"raw\", \"spectral\", or \"topo\"\n",
    "        one_channel: Whether input has 1 or 3 channel in depth dimension. Matters when load topo data as number of input channels \n",
    "                are different from original's\n",
    "        version: Any additional information of the datafile. Will be appended to the file name at the end\n",
    "    \"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    x = f[f'X_{role}']\n",
    "    if feature == 'raw':\n",
    "        x = np.transpose(x,(0,2,1))\n",
    "        x = np.reshape(x,(-1,1,numChan,winLength*srate))\n",
    "    elif feature == 'topo':\n",
    "        if one_channel:\n",
    "            samples = []\n",
    "            for i in range(x.shape[0]):\n",
    "                image = x[i]\n",
    "                b, g, r = image[0,:, :], image[1,:, :], image[2,:, :]\n",
    "                concat = np.concatenate((b,g,r), axis=1)\n",
    "                samples.append(concat)\n",
    "            x = np.stack(samples)\n",
    "            x = np.reshape(x,(-1,1,x.shape[1],x.shape[2]))\n",
    "    \n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_yclass1_ages_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_yclass1_ages_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    y = np.subtract(f[f'Y_cls_{role}'], 1)\n",
    "   \n",
    "    return x,y\n",
    "\n",
    "\n",
    "\n",
    "def plot_to_image_tensor(figure):\n",
    "    # Save the plot to a PNG in memory.\n",
    "    figure.savefig('batch.png')\n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    img = Image.open('batch.png')\n",
    "    trans = transforms.ToPILImage()\n",
    "    trans1 = transforms.ToTensor()\n",
    "    image_tensor = trans1(img)\n",
    "    return image_tensor\n",
    "\n",
    "def plot_EEG(data, feature, numChan, one_channel=True):\n",
    "    '''\n",
    "    Plot EEG sample\n",
    "    :param\n",
    "        data: An EEGDataset object\n",
    "        feature: String - 'raw' or 'topo'\n",
    "        numChan: Int - number of EEG channels\n",
    "        one_channel: Bool - Whether input has 1 or 3 channel in depth dimension. Matters when load topo data as number of input channels \n",
    "                are different from original's\n",
    "    '''\n",
    "    x_data = data[:][0]\n",
    "    if feature == 'raw':        \n",
    "        fig = plt.figure(figsize=(80, 80))\n",
    "        outer = gridspec.GridSpec(8, 8)\n",
    "        for i in range(64):\n",
    "            inner = gridspec.GridSpecFromSubplotSpec(numChan, 1,\n",
    "                            subplot_spec=outer[i])\n",
    "#             npimg = img[i,:,:,:].numpy()\n",
    "            npimg = x_data[i,:,:,:]\n",
    "            npimg = np.reshape(npimg,(24,256))\n",
    "            yax = None\n",
    "            for j in range(24):\n",
    "                ax = plt.Subplot(fig, inner[j])\n",
    "                ax.plot(range(256),npimg[j,:],'k')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                fig.add_subplot(ax)\n",
    "\n",
    "        return fig\n",
    "    else:\n",
    "        sample = 2\n",
    "        if one_channel:\n",
    "            image = np.reshape(x_data[sample], (x_data[sample].shape[1],x_data[sample].shape[2]))\n",
    "            plt.imshow(image.astype('int32'))\n",
    "        else:\n",
    "            plt.imshow(np.transpose(x_data[sample].astype('int32'), (1, 2, 0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EEG data\n",
    "path = './data/'\n",
    "winLength = 2\n",
    "numChan = 24\n",
    "srate = 128\n",
    "feature = 'raw'\n",
    "one_channel = False\n",
    "\n",
    "role = 'train'\n",
    "train_data_x, train_data_labels = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n",
    "# print(f'X_train shape: {len(train_data)}, {train_data[0][0].shape}')\n",
    "# print(f'Y_train shape: {len(train_data)}, {train_data[0][1].shape}')\n",
    "\n",
    "role = 'val'\n",
    "val_data_x, val_data_labels = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n",
    "# print(f'X_val shape: {len(val_data)}, {val_data[0][0].shape}')\n",
    "# print(f'Y_val shape: {len(val_data)}, {val_data[0][1].shape}')\n",
    "# EEGDataset(x, y, role=='train', role=='val')\n",
    "# plot_EEG(train_data, feature, numChan, one_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_data_labels.copy()\n",
    "train_data_y[np.logical_and(train_data_labels <= 2,train_data_labels >= 0)] = 0\n",
    "train_data_y[train_data_labels > 2] = 1\n",
    "val_data_y = val_data_labels.copy()\n",
    "val_data_y[np.logical_and(val_data_labels <= 2,val_data_labels >= 0)] = 0\n",
    "val_data_y[val_data_labels > 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([43597.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0., 27784.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.610764769336378"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43597/(43597+27784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24980.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0., 14888.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(val_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6265676733219625"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24980/(24980+14888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EEGDataset(train_data_x, train_data_y, True, False)\n",
    "val_data = EEGDataset(val_data_x, val_data_y, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize device information for PyTorch\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    '''\n",
    "    Check accuracy of the model \n",
    "    param:\n",
    "        loader: An EEGDataset object\n",
    "        model: A PyTorch Module to test\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        logger.log('Checking accuracy on training set')\n",
    "    elif loader.dataset.val:\n",
    "        logger.log('Checking accuracy on validation set')\n",
    "    else:\n",
    "        logger.log('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            scores = model(x)\n",
    "#             print(f\"label {y}, prediction {scores}\")\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        logger.log('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, loader_val, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model using the PyTorch Module API.\n",
    "    \n",
    "    params:\n",
    "        model: A PyTorch Module giving the model to train.\n",
    "        loader_train: A PyTorch DataLoader object containing training data loaded in batch\n",
    "        loader_val: A PyTorch DataLoader object containing validation data loaded in batch        \n",
    "        optimizer: An Optimizer object we will use to train the model\n",
    "        epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "#     mseLoss = nn.MSELoss()\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "#             loss = mseLoss(scores, y)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                logger.writer.add_scalar(\"Loss/train\", loss.item(), e*len(loader_train)+t)\n",
    "                logger.log('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "        train_acc = check_accuracy(loader_train, model)\n",
    "        logger.writer.add_scalar(\"Acc/train\", train_acc, e)        \n",
    "        val_acc = check_accuracy(loader_val, model)\n",
    "        logger.writer.add_scalar(\"Acc/valid\", val_acc, e)        \n",
    "        logger.log()\n",
    "        \n",
    "        # Save model every 20 epochs\n",
    "        if e > 0 and e % 10 == 0:\n",
    "            logger.save_model(model,f\"epoch{e}\")\n",
    "        elif val_acc >= 0.7:\n",
    "            logger.save_model(model,f\"valacc70-epoch{e}\")\n",
    "        elif val_acc >= 0.71:\n",
    "            logger.save_model(model,f\"valacc71-epoch{e}\")\n",
    "    # save final model\n",
    "    logger.save_model(model,f\"epoch{e}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_x = train_data_x.reshape((71381, 24, 256))\n",
    "val_data_x = val_data_x.reshape((39868, 24, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EEGDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-90df03dce17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'EEGDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_data.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version=1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"tf version={tf.__version__}\")\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_41 (Conv1D)           (None, 24, 64)            49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 12, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 64)             256       \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 6, 128)            24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 3, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 2, 128)            512       \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 563,714\n",
      "Trainable params: 563,330\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(24, 256)))\n",
    "model.add(MaxPooling1D(2, 2, padding='same'))\n",
    "model.add((Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(MaxPooling1D(2, 2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add((Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(MaxPooling1D(2, 2, padding='same'))\n",
    "model.add((Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')))\n",
    "model.add(MaxPooling1D(2, 2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add((Dropout(0.3)))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71381 samples, validate on 39868 samples\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "71381/71381 [==============================] - 34s 478us/sample - loss: 9.8340 - acc: 0.3896 - val_loss: 10.0991 - val_acc: 0.3734\n",
      "Epoch 2/50\n",
      "71381/71381 [==============================] - 32s 446us/sample - loss: 9.8444 - acc: 0.3892 - val_loss: 10.0991 - val_acc: 0.3734\n",
      "Epoch 3/50\n",
      "71381/71381 [==============================] - 32s 447us/sample - loss: 9.8444 - acc: 0.3892 - val_loss: 10.0991 - val_acc: 0.3734\n",
      "Epoch 4/50\n",
      "15104/71381 [=====>........................] - ETA: 20s - loss: 9.7718 - acc: 0.3937"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-274c2d93d808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_data_x, train_data_y, batch_size=64, epochs=50, validation_data=(val_data_x, val_data_y), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(val_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = [p[0]<p[1] for p in val_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71381, 24, 256, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x = np.swapaxes(train_data_x, 1, 3)\n",
    "train_data_x = np.swapaxes(train_data_x, 1, 2)\n",
    "train_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_x = np.swapaxes(val_data_x, 1, 3)\n",
    "val_data_x = np.swapaxes(val_data_x, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = tf.keras.utils.to_categorical(train_data_y)\n",
    "val_data_y = tf.keras.utils.to_categorical(val_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 22, 254, 100)      1000      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 11, 127, 100)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 11, 127, 100)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 125, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 62, 100)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 62, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 60, 300)        180300    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 30, 300)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 30, 300)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 24, 300)        630300    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 23, 300)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 23, 300)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 21, 100)        90100     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 19, 100)        30100     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1900)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6144)              11679744  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 12290     \n",
      "=================================================================\n",
      "Total params: 12,713,934\n",
      "Trainable params: 12,713,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(100, 3, activation='relu', input_shape=(24, 256, 1)))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(100, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(300, (2, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(300, (1, 7), activation='relu'))\n",
    "model.add(MaxPooling2D((1, 2), 1))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(100, (1, 3), activation='relu'))\n",
    "model.add(Conv2D(100, (1, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6144, activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"relu\"))\n",
    "\n",
    "adam = keras.optimizers.Adam(learning_rate=0.002)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    '''\n",
    "    Create the CNN following configuration in van Putten et al. (2018)\n",
    "    '''\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(1,100,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(100,100,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(100,300,(2,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(300,300,(1,7)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1,2), stride=1),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(300,100,(1,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(100,100,(1,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1900,6144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6144,2),\n",
    "        )\n",
    "    return model\n",
    "#     feature = 'raw'\n",
    "#     subsample = 4\n",
    "#     tmp = models.vgg16()\n",
    "#     tmp.features = tmp.features[0:17]\n",
    "#     vgg16_rescaled = nn.Sequential()\n",
    "#     modules = []\n",
    "    \n",
    "#     if feature == 'raw':\n",
    "#         first_in_channels = 1\n",
    "#         first_in_features = 6144\n",
    "#     else:\n",
    "#         first_in_channels = 3\n",
    "#         first_in_features = 576\n",
    "        \n",
    "#     for layer in tmp.features.children():\n",
    "#         if isinstance(layer, nn.Conv2d):\n",
    "#             if layer.in_channels == 3:\n",
    "#                 in_channels = first_in_channels\n",
    "#             else:\n",
    "#                 in_channels = int(layer.in_channels/subsample)\n",
    "#             out_channels = int(layer.out_channels/subsample)\n",
    "#             modules.append(nn.Conv2d(in_channels, out_channels, layer.kernel_size, layer.stride, layer.padding))\n",
    "#         else:\n",
    "#             modules.append(layer)\n",
    "#     vgg16_rescaled.add_module('features',nn.Sequential(*modules))\n",
    "#     vgg16_rescaled.add_module('flatten', nn.Flatten())\n",
    "\n",
    "#     modules = []\n",
    "#     for layer in tmp.classifier.children():\n",
    "#         if isinstance(layer, nn.Linear):\n",
    "#             if layer.in_features == 25088:\n",
    "#                 in_features = first_in_features\n",
    "#             else:\n",
    "#                 in_features = int(layer.in_features/subsample) \n",
    "#             if layer.out_features == 1000:\n",
    "#                 out_features = 2\n",
    "#             else:\n",
    "#                 out_features = int(layer.out_features/subsample) \n",
    "#             modules.append(nn.Linear(in_features, out_features))\n",
    "#         else:\n",
    "#             modules.append(layer)\n",
    "#     vgg16_rescaled.add_module('classifier', nn.Sequential(*modules))\n",
    "#     return vgg16_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-model-summary in /home/yiy287/.local/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch-model-summary) (4.32.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch-model-summary) (1.16.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pytorch-model-summary) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-model-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and show model summary\n",
    "model = create_model()\n",
    "#from pytorch_model_summary import summary\n",
    "#print(summary(model, torch.zeros((1, 1, 24, 256)), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, subj_csv):\n",
    "    '''\n",
    "    Test model using two different metrics. First is per sample accuracy.\n",
    "    Second is to use 40 samples per subject and perform voting:\n",
    "        If mean prediction > 0.5, classify as female (1)\n",
    "        Otherwise, classify as male (0)\n",
    "    param:\n",
    "        model: A trained PyTorch Module\n",
    "        test_data: test dataset\n",
    "        subj_csv: spreadsheet containing subject IDs for the test dataset\n",
    "    '''\n",
    "    # one-segment test\n",
    "    logger.log('Testing model accuracy using 1-segment metric')\n",
    "    loader_test = DataLoader(test_data, batch_size=70)\n",
    "    per_sample_acc = check_accuracy(loader_test, model)\n",
    "\n",
    "    # 40-segment test\n",
    "    logger.log('Testing model accuracy using 40-segment per subject metric')\n",
    "    with open(subj_csv, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        subjIDs = [row[0] for row in spamreader]\n",
    "    unique_subjs,indices = np.unique(subjIDs,return_index=True)\n",
    "\n",
    "    iterable_test_data = list(iter(DataLoader(test_data, batch_size=1)))\n",
    "    num_correct = []\n",
    "    for subj,idx in zip(unique_subjs,indices):\n",
    "    #     print(f'Subj {subj} - gender {iterable_test_data[idx][1]}')\n",
    "        data = iterable_test_data[idx:idx+40]\n",
    "        #print(np.sum([y for _,y in data]))\n",
    "        assert 40 == np.sum([y for _,y in data]) or 0 == np.sum([y for _,y in data])\n",
    "        preds = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in data:\n",
    "                x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "                correct = y\n",
    "                scores = model(x)\n",
    "                _, pred = scores.max(1)\n",
    "                preds.append(pred)\n",
    "        final_pred = (torch.mean(torch.FloatTensor(preds)) > 0.5).sum()\n",
    "        num_correct.append((final_pred == correct).sum())\n",
    "    #print(len(num_correct))\n",
    "    acc = float(np.sum(num_correct)) / len(unique_subjs)\n",
    "    logger.log('Got %d / %d correct (%.2f)' % (np.sum(num_correct), len(unique_subjs), 100 * acc))\n",
    "    return per_sample_acc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed, model_name, feature, loader_train, loader_val, num_epoch):\n",
    "    '''\n",
    "    Train a model for num_epoch given a random seed. \n",
    "    During training, logs and intemediary models will be saved in files accordingly to model_name\n",
    "    param:\n",
    "        seed: Int - Random seed number\n",
    "        model_name: String - Name of the model to be saved. Used for logging\n",
    "        feature: String - Whether 'raw' or 'topo'\n",
    "        loader_train: DataLoader with training set\n",
    "        loader_val: DataLoader with validation set\n",
    "        num_epoch: Int - number of epoch to train the model\n",
    "    '''\n",
    "    model = create_model()\n",
    "    logger.set_model_save_location(f'{model_name}-{feature}')\n",
    "    experiment = f'{model_name}-{feature}-seed{seed}'\n",
    "    logger.set_experiment(experiment)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # toggle between learning rate and batch size values \n",
    "\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=0.002)\n",
    "    model = train(model, loader_train, loader_val, optimizer, epochs=num_epoch)\n",
    "    \n",
    "    # Testing\n",
    "    logger.log('Testing on balanced test set')\n",
    "    test_data_balanced = load_data(path, 'test', winLength, numChan, srate, feature)\n",
    "    sample_acc1, subject_acc1 = test_model(model, test_data_balanced, path + 'test_subjIDs_fewer_subjects.csv')\n",
    "\n",
    "    logger.log('Testing on all-male test set')\n",
    "    test_data_all_male = load_data(path, 'test', winLength, numChan, srate, feature,'v3')\n",
    "    sample_acc2, subject_acc2 = test_model(model, test_data_all_male, path + 'test_subjIDs_more_test.csv')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "ZSqENpKfB2UE",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size 100 1 3, expected input[256, 24, 256] to have 1 channels, but got 24 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-37dae0705674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloader_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'original-ages-classification'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# model = run_experiment(9, 'original', 'raw',70)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-45482a0dee14>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(seed, model_name, feature, loader_train, loader_val, num_epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-8fea72d66609>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader_train, loader_val, optimizer, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#             loss = mseLoss(scores, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size 100 1 3, expected input[256, 24, 256] to have 1 channels, but got 24 channels instead"
     ]
    }
   ],
   "source": [
    "# train model multiple times, each with different random seed\n",
    "logger = Logger() # initialize logger to be used throughout\n",
    "\n",
    "batch_size = 256\n",
    "loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(val_data, batch_size=batch_size)\n",
    "for s in range(1):\n",
    "    model = run_experiment(s, 'original-ages-classification', 'raw',loader_train, loader_val,70)\n",
    "# model = run_experiment(9, 'original', 'raw',70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x, test_data_labels = load_data(path, 'test', winLength, numChan, srate, feature, one_channel)\n",
    "test_data_y = test_data_labels.copy()\n",
    "test_data_y[np.logical_and(test_data_labels <= 2,test_data_labels >= 0)] = 0\n",
    "test_data_y[test_data_labels > 2] = 1\n",
    "test_data = EEGDataset(test_data_x, test_data_y, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved-model/original-ages-classification-raw/model-original-ages-classification-raw-seed0-valacc70-epoch49'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 11501 / 15925 correct (72.22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7221978021978022"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(mode='debug')\n",
    "loader_test = DataLoader(test_data, batch_size=1)\n",
    "model.to(device=device)\n",
    "check_accuracy(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8797.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        7128.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5524018838304553"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8797/(8797+7128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_seeds(partial_model_path, epoch, num_seed, isBalanced=True):\n",
    "    '''\n",
    "    Given name of saved model and epoch, check test accuracy for all random seeds in range(num_seed)\n",
    "    \n",
    "    '''\n",
    "    if isBalanced:\n",
    "        logger.log('Testing on balanced test set')\n",
    "        test_data = load_data(path, 'test', winLength, numChan, srate, feature,'v2')\n",
    "        subjIDs_file = 'data/test_subjIDs_fewer_subjects.csv'\n",
    "    else:\n",
    "        logger.log('Testing on all male test set')\n",
    "        test_data = load_data(path, 'test', winLength, numChan, srate, feature,'v3')\n",
    "        subjIDs_file = 'data/test_subjIDs_more_test.csv'\n",
    "\n",
    "    sample_acc = []\n",
    "    subject_acc = []\n",
    "    for s in range(num_seed):\n",
    "        model = create_model()\n",
    "        model.load_state_dict(torch.load(f'{partial_model_path}-seed{s}-epoch{epoch}'))\n",
    "        model.to(device=device)\n",
    "        sam_acc, sub_acc = test_model(model, test_data,subjIDs_file)\n",
    "        sample_acc.append(sam_acc)\n",
    "        subject_acc.append(sub_acc)\n",
    "        \n",
    "    sample_acc = np.multiply(sample_acc,100)\n",
    "    subject_acc = np.multiply(subject_acc,100)\n",
    "    return sample_acc, subject_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on balanced test set\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13172 / 15925 correct (82.71)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 170 / 197 correct (86.29)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13102 / 15925 correct (82.27)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 169 / 197 correct (85.79)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12874 / 15925 correct (80.84)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 168 / 197 correct (85.28)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12761 / 15925 correct (80.13)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 165 / 197 correct (83.76)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12477 / 15925 correct (78.35)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 163 / 197 correct (82.74)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12681 / 15925 correct (79.63)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 165 / 197 correct (83.76)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13115 / 15925 correct (82.35)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 169 / 197 correct (85.79)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12831 / 15925 correct (80.57)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 170 / 197 correct (86.29)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12487 / 15925 correct (78.41)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 167 / 197 correct (84.77)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12899 / 15925 correct (81.00)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 171 / 197 correct (86.80)\n",
      "Per sample\n",
      "Min: 78.34850863422292, Max: 82.712715855573, Mean: 80.62731554160125, STDEV: 1.4687850472648853\n",
      "Per subject\n",
      "Min: 82.74111675126903, Max: 86.80203045685279, Mean: 85.1269035532995, STDEV: 1.2649681009519929\n",
      "Testing on balanced test set\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12652 / 15925 correct (79.45)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 166 / 197 correct (84.26)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12309 / 15925 correct (77.29)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 163 / 197 correct (82.74)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12656 / 15925 correct (79.47)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 169 / 197 correct (85.79)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13112 / 15925 correct (82.34)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 168 / 197 correct (85.28)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12905 / 15925 correct (81.04)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 166 / 197 correct (84.26)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13040 / 15925 correct (81.88)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 172 / 197 correct (87.31)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13256 / 15925 correct (83.24)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 170 / 197 correct (86.29)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12843 / 15925 correct (80.65)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 168 / 197 correct (85.28)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12972 / 15925 correct (81.46)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 171 / 197 correct (86.80)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13330 / 15925 correct (83.70)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 175 / 197 correct (88.83)\n",
      "Per sample\n",
      "Min: 77.29356357927787, Max: 83.70486656200941, Mean: 81.05180533751962, STDEV: 1.8354826738149437\n",
      "Per subject\n",
      "Min: 82.74111675126903, Max: 88.83248730964468, Mean: 85.68527918781726, STDEV: 1.6495509451037509\n",
      "Testing on balanced test set\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12996 / 15925 correct (81.61)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 170 / 197 correct (86.29)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12924 / 15925 correct (81.16)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 171 / 197 correct (86.80)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12807 / 15925 correct (80.42)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 166 / 197 correct (84.26)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13038 / 15925 correct (81.87)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 163 / 197 correct (82.74)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13212 / 15925 correct (82.96)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 173 / 197 correct (87.82)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13123 / 15925 correct (82.41)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 171 / 197 correct (86.80)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13146 / 15925 correct (82.55)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 171 / 197 correct (86.80)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12114 / 15925 correct (76.07)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 157 / 197 correct (79.70)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12988 / 15925 correct (81.56)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 167 / 197 correct (84.77)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13016 / 15925 correct (81.73)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 172 / 197 correct (87.31)\n",
      "Per sample\n",
      "Min: 76.0690737833595, Max: 82.96389324960754, Mean: 81.23328100470958, STDEV: 1.8530402645497464\n",
      "Per subject\n",
      "Min: 79.69543147208121, Max: 87.81725888324873, Mean: 85.32994923857868, STDEV: 2.3965627683477937\n",
      "Testing on balanced test set\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13132 / 15925 correct (82.46)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 170 / 197 correct (86.29)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12803 / 15925 correct (80.40)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 170 / 197 correct (86.29)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12988 / 15925 correct (81.56)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 173 / 197 correct (87.82)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13138 / 15925 correct (82.50)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 174 / 197 correct (88.32)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13099 / 15925 correct (82.25)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 175 / 197 correct (88.83)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13004 / 15925 correct (81.66)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 169 / 197 correct (85.79)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12951 / 15925 correct (81.32)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 168 / 197 correct (85.28)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12758 / 15925 correct (80.11)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 165 / 197 correct (83.76)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 13029 / 15925 correct (81.81)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 166 / 197 correct (84.26)\n",
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Got 12702 / 15925 correct (79.76)\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 166 / 197 correct (84.26)\n",
      "Per sample\n",
      "Min: 79.76138147566719, Max: 82.49921507064364, Mean: 81.3839874411303, STDEV: 0.9317428732667018\n",
      "Per subject\n",
      "Min: 83.75634517766497, Max: 88.83248730964468, Mean: 86.09137055837564, STDEV: 1.686624134602657\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(mode='debug')\n",
    "# Compute test performance statistics for all intermidiary saved-models at each specified epochs\n",
    "# and save result to a csv file\n",
    "epochs = [40, 50, 60, 69]\n",
    "\n",
    "with open(\"original-relu-raw-test-results-balanced.csv\", 'w') as out:\n",
    "    out.write('epoch,min_sam,max_sam,mean_sam,std_sam,min_subj,max_subj,mean_subj,std_subj\\n')\n",
    "    for epoch in epochs:\n",
    "        sample_acc, subject_acc = test_all_seeds(partial_model_path=\"saved-model/original-relu-raw/model-original-relu-raw\", epoch=epoch, num_seed=10, isBalanced=True)\n",
    "\n",
    "        min_sample = np.min(sample_acc)\n",
    "        max_sample = np.max(sample_acc)\n",
    "        mean_sample = np.mean(sample_acc)\n",
    "        std_sample = np.std(sample_acc)\n",
    "\n",
    "        min_subj = np.min(subject_acc)\n",
    "        max_subj = np.max(subject_acc)\n",
    "        mean_subj = np.mean(subject_acc)\n",
    "        std_subj = np.std(subject_acc)\n",
    "\n",
    "        logger.log(\"Per sample\")\n",
    "        logger.log(f\"Min: {min_sample}, Max: {max_sample}, Mean: {mean_sample}, STDEV: {std_sample}\")\n",
    "\n",
    "        logger.log(\"Per subject\")\n",
    "        logger.log(f\"Min: {min_subj}, Max: {max_subj}, Mean: {mean_subj}, STDEV: {std_subj}\")\n",
    "        out.write(f\"{epoch},{min_sample},{max_sample},{mean_sample},{std_sample},{min_subj},{max_subj},{mean_subj},{std_subj}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data_x.reshape((71381, 24*256))\n",
    "train_y = train_data_y.reshape((71381))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32min 5s, sys: 7.45 s, total: 32min 12s\n",
      "Wall time: 32min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=500, random_state=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "n_train = 5000\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(train_x[:n_train], train_y[:n_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.9686\n",
      "val acc = 0.6268435838266279\n",
      "test acc = 0.5940973312401884\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf.predict(train_x[:n_train])\n",
    "accu_train = sum(train_pred == train_y[:n_train]) / n_train\n",
    "print(f'train acc = {accu_train}')\n",
    "\n",
    "val_pred = clf.predict(val_data_x.reshape((39868, 24*256)))\n",
    "val_y = val_data_y.reshape((39868))\n",
    "accu_val = sum(val_pred == val_y) / len(val_y)\n",
    "print(f'val acc = {accu_val}')\n",
    "\n",
    "test_pred = clf.predict(test_data_x.reshape((15925, 24*256)))\n",
    "test_y = test_data_y.reshape((15925))\n",
    "accu_val = sum(test_pred == test_y) / len(test_y)\n",
    "print(f'test acc = {accu_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.389235230663622\n",
      "0.3734323266780375\n",
      "0.44759811616954476\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_y)/len(train_y))\n",
    "print(sum(val_y)/len(val_y))\n",
    "print(sum(test_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.2 s, sys: 139 ms, total: 38.3 s\n",
      "Wall time: 38.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "n_train = 10000\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(train_x[:n_train], train_y[:n_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc = 0.6691\n",
      "val acc = 0.6297782682853417\n",
      "test acc = 0.6087912087912087\n"
     ]
    }
   ],
   "source": [
    "train_pred = clf.predict(train_x[:n_train])\n",
    "accu_train = sum(train_pred == train_y[:n_train]) / n_train\n",
    "print(f'train acc = {accu_train}')\n",
    "\n",
    "val_pred = clf.predict(val_data_x.reshape((39868, 24*256)))\n",
    "val_y = val_data_y.reshape((39868))\n",
    "accu_val = sum(val_pred == val_y) / len(val_y)\n",
    "print(f'val acc = {accu_val}')\n",
    "\n",
    "test_pred = clf.predict(test_data_x.reshape((15925, 24*256)))\n",
    "test_y = test_data_y.reshape((15925))\n",
    "accu_val = sum(test_pred == test_y) / len(test_y)\n",
    "print(f'test acc = {accu_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SexPrediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
