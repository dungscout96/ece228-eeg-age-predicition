{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2c0_0uImBwm5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# For visualize input\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    '''\n",
    "    Custom Dataset object for PyTorch to load the dataset\n",
    "    '''\n",
    "    def __init__(self, x, y, train, val):\n",
    "        super(EEGDataset).__init__()\n",
    "        assert x.shape[0] == y.size\n",
    "        self.x = x\n",
    "        self.y = [y[i][0] for i in range(y.size)]\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "\n",
    "    def __getitem__(self,key):\n",
    "        return (self.x[key], self.y[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "class Logger():\n",
    "    '''\n",
    "    Object controlling how information will be logged\n",
    "    A logger created globally will be used to log all information\n",
    "    Create a Logger(mode='debug') to have everything print to the console\n",
    "    '''\n",
    "    def __init__(self, mode='log'):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def set_model_save_location(self, model_dir):\n",
    "        self.model_dir = f\"saved-model/{model_dir}\"\n",
    "        if not os.path.isdir(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "        \n",
    "    def set_experiment(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        log_format = '%(asctime)s %(message)s'\n",
    "        logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                            format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "        fh = logging.FileHandler(os.path.join('training-logs', f'log-{experiment_name}-{datetime.datetime.today()}.txt'))\n",
    "        fh.setFormatter(logging.Formatter(log_format))\n",
    "        logging.getLogger().addHandler(fh)\n",
    "        self.writer = SummaryWriter(f\"runs/{experiment_name}\")\n",
    "            \n",
    "    def log(self, message=\"\"):\n",
    "        if self.mode == 'log':\n",
    "            logging.info(message)\n",
    "        elif self.mode == 'debug':\n",
    "            print(message)\n",
    "\n",
    "    def save_model(self, model, info):\n",
    "        torch.save(model.state_dict(), f\"{self.model_dir}/model-{logger.experiment_name}-{info}\")\n",
    "        \n",
    "def load_data(path, role, winLength, numChan, srate, feature, one_channel=False, version=\"\"):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    :param  \n",
    "        path: Filepath to the dataset\n",
    "        role: Role of the dataset. Can be \"train\", \"val\", or \"test\"\n",
    "        winLength: Length of time window. Can be 2 or 15\n",
    "        numChan: Number of channels. Can be 24 or 128\n",
    "        srate: Sampling rate. Supporting 126Hz\n",
    "        feature: Input feature. Can be \"raw\", \"spectral\", or \"topo\"\n",
    "        one_channel: Whether input has 1 or 3 channel in depth dimension. Matters when load topo data as number of input channels \n",
    "                are different from original's\n",
    "        version: Any additional information of the datafile. Will be appended to the file name at the end\n",
    "    \"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    x = f[f'X_{role}']\n",
    "    if feature == 'raw':\n",
    "        x = np.transpose(x,(0,2,1))\n",
    "        x = np.reshape(x,(-1,1,numChan,winLength*srate))\n",
    "    elif feature == 'topo':\n",
    "        if one_channel:\n",
    "            samples = []\n",
    "            for i in range(x.shape[0]):\n",
    "                image = x[i]\n",
    "                b, g, r = image[0,:, :], image[1,:, :], image[2,:, :]\n",
    "                concat = np.concatenate((b,g,r), axis=1)\n",
    "                samples.append(concat)\n",
    "            x = np.stack(samples)\n",
    "            x = np.reshape(x,(-1,1,x.shape[1],x.shape[2]))\n",
    "    \n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_yclass1_ages_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_yclass1_ages_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    y = np.subtract(f[f'Y_cls_{role}'], 1)\n",
    "   \n",
    "    return x,y\n",
    "\n",
    "\n",
    "\n",
    "def plot_to_image_tensor(figure):\n",
    "    # Save the plot to a PNG in memory.\n",
    "    figure.savefig('batch.png')\n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    img = Image.open('batch.png')\n",
    "    trans = transforms.ToPILImage()\n",
    "    trans1 = transforms.ToTensor()\n",
    "    image_tensor = trans1(img)\n",
    "    return image_tensor\n",
    "\n",
    "def plot_EEG(data, feature, numChan, one_channel=True):\n",
    "    '''\n",
    "    Plot EEG sample\n",
    "    :param\n",
    "        data: An EEGDataset object\n",
    "        feature: String - 'raw' or 'topo'\n",
    "        numChan: Int - number of EEG channels\n",
    "        one_channel: Bool - Whether input has 1 or 3 channel in depth dimension. Matters when load topo data as number of input channels \n",
    "                are different from original's\n",
    "    '''\n",
    "    x_data = data[:][0]\n",
    "    if feature == 'raw':        \n",
    "        fig = plt.figure(figsize=(80, 80))\n",
    "        outer = gridspec.GridSpec(8, 8)\n",
    "        for i in range(64):\n",
    "            inner = gridspec.GridSpecFromSubplotSpec(numChan, 1,\n",
    "                            subplot_spec=outer[i])\n",
    "#             npimg = img[i,:,:,:].numpy()\n",
    "            npimg = x_data[i,:,:,:]\n",
    "            npimg = np.reshape(npimg,(24,256))\n",
    "            yax = None\n",
    "            for j in range(24):\n",
    "                ax = plt.Subplot(fig, inner[j])\n",
    "                ax.plot(range(256),npimg[j,:],'k')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                fig.add_subplot(ax)\n",
    "\n",
    "        return fig\n",
    "    else:\n",
    "        sample = 2\n",
    "        if one_channel:\n",
    "            image = np.reshape(x_data[sample], (x_data[sample].shape[1],x_data[sample].shape[2]))\n",
    "            plt.imshow(image.astype('int32'))\n",
    "        else:\n",
    "            plt.imshow(np.transpose(x_data[sample].astype('int32'), (1, 2, 0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EEG data\n",
    "path = './data/'\n",
    "winLength = 2\n",
    "numChan = 24\n",
    "srate = 128\n",
    "feature = 'raw'\n",
    "one_channel = False\n",
    "\n",
    "role = 'train'\n",
    "train_data_x, train_data_labels = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n",
    "\n",
    "role = 'val'\n",
    "val_data_x, val_data_labels = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: 71381, (1, 24, 256)\n",
      "Y_train shape: 71381, ()\n",
      "X_val shape: 39868, (1, 24, 256)\n",
      "Y_val shape: 39868, ()\n"
     ]
    }
   ],
   "source": [
    "# transform loaded data to PyTorch Dataset\n",
    "train_data = EEGDataset(train_data_x, train_data_labels, True, False)\n",
    "val_data = EEGDataset(val_data_x, val_data_labels, False, True)\n",
    "print(f'X_train shape: {len(train_data)}, {train_data[0][0].shape}')\n",
    "print(f'Y_train shape: {len(train_data)}, {train_data[0][1].shape}')\n",
    "print(f'X_val shape: {len(val_data)}, {val_data[0][0].shape}')\n",
    "print(f'Y_val shape: {len(val_data)}, {val_data[0][1].shape}')\n",
    "\n",
    "# Visualize input\n",
    "plot_EEG(train_data, feature, numChan, one_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group subjects into child and adolescent age groups\n",
    "train_data_y = train_data_labels.copy()\n",
    "train_data_y[np.logical_and(train_data_labels <= 2,train_data_labels >= 0)] = 0\n",
    "train_data_y[train_data_labels > 2] = 1\n",
    "val_data_y = val_data_labels.copy()\n",
    "val_data_y[np.logical_and(val_data_labels <= 2,val_data_labels >= 0)] = 0\n",
    "val_data_y[val_data_labels > 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent group 0 in training set: 0.610764769336378\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_data_y)\n",
    "print(f'Percent group 0 in training set: {(train_data_y==0).sum()/len(train_data_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent group 0 in validation set: 0.6265676733219625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(val_data_y)\n",
    "print(f'Percent group 0 in validation set: {(val_data_y==0).sum()/len(val_data_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize device information for PyTorch\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    '''\n",
    "    Check accuracy of the model \n",
    "    param:\n",
    "        loader: An EEGDataset object\n",
    "        model: A PyTorch Module to test\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        logger.log('Checking accuracy on training set')\n",
    "    elif loader.dataset.val:\n",
    "        logger.log('Checking accuracy on validation set')\n",
    "    else:\n",
    "        logger.log('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    lossTotal = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y, weight=torch.tensor([class_0_weight,class_1_weight], dtype=dtype,device=device))\n",
    "            lossTotal += loss.item()\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        lossTotal = lossTotal / len(loader)\n",
    "        logger.log('Loss (%f)' % (loss))\n",
    "        logger.log('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "        return acc,lossTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_weight = (train_data_y==0).sum()/len(train_data_y)\n",
    "class_1_weight = 1-class_0_weight\n",
    "def train(model, loader_train, loader_val, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model using the PyTorch Module API.\n",
    "    \n",
    "    params:\n",
    "        model: A PyTorch Module giving the model to train.\n",
    "        loader_train: A PyTorch DataLoader object containing training data loaded in batch\n",
    "        loader_val: A PyTorch DataLoader object containing validation data loaded in batch        \n",
    "        optimizer: An Optimizer object we will use to train the model\n",
    "        epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y, weight=torch.tensor([class_0_weight,class_1_weight], dtype=dtype,device=device))\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                logger.log('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "        train_acc, train_loss = check_accuracy(loader_train, model)\n",
    "        logger.writer.add_scalar(\"Loss/train\", train_loss, e*len(loader_train)+t)\n",
    "        logger.writer.add_scalar(\"Acc/train\", train_acc, e)        \n",
    "        val_acc, val_loss = check_accuracy(loader_val, model)\n",
    "        logger.writer.add_scalar(\"Acc/val\", val_acc, e)        \n",
    "        logger.writer.add_scalar(\"Loss/val\", val_loss, e*len(loader_train)+t)\n",
    "        logger.log()\n",
    "        \n",
    "        # Save model every 20 epochs\n",
    "        if e > 0 and e % 10 == 0:\n",
    "            logger.save_model(model,f\"epoch{e}\")\n",
    "        elif val_acc >= 0.7:\n",
    "            logger.save_model(model,f\"valacc{round(val_acc*100,2)}-epoch{e}\")\n",
    "    # save final model\n",
    "    logger.save_model(model,f\"epoch{e}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    '''\n",
    "    Create the CNN following configuration in van Putten et al. (2018)\n",
    "    '''\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(1,100,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(100,100,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(100,300,(2,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(300,300,(1,7)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1,2), stride=1),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(300,100,(1,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(100,100,(1,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1900,6144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6144,2),\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Conv2d-1     [1, 100, 22, 254]           1,000           1,000\n",
      "            ReLU-2     [1, 100, 22, 254]               0               0\n",
      "       MaxPool2d-3     [1, 100, 11, 127]               0               0\n",
      "         Dropout-4     [1, 100, 11, 127]               0               0\n",
      "          Conv2d-5      [1, 100, 9, 125]          90,100          90,100\n",
      "            ReLU-6      [1, 100, 9, 125]               0               0\n",
      "       MaxPool2d-7       [1, 100, 4, 62]               0               0\n",
      "         Dropout-8       [1, 100, 4, 62]               0               0\n",
      "          Conv2d-9       [1, 300, 3, 60]         180,300         180,300\n",
      "           ReLU-10       [1, 300, 3, 60]               0               0\n",
      "      MaxPool2d-11       [1, 300, 1, 30]               0               0\n",
      "        Dropout-12       [1, 300, 1, 30]               0               0\n",
      "         Conv2d-13       [1, 300, 1, 24]         630,300         630,300\n",
      "           ReLU-14       [1, 300, 1, 24]               0               0\n",
      "      MaxPool2d-15       [1, 300, 1, 23]               0               0\n",
      "        Dropout-16       [1, 300, 1, 23]               0               0\n",
      "         Conv2d-17       [1, 100, 1, 21]          90,100          90,100\n",
      "           ReLU-18       [1, 100, 1, 21]               0               0\n",
      "         Conv2d-19       [1, 100, 1, 19]          30,100          30,100\n",
      "           ReLU-20       [1, 100, 1, 19]               0               0\n",
      "        Flatten-21             [1, 1900]               0               0\n",
      "         Linear-22             [1, 6144]      11,679,744      11,679,744\n",
      "           ReLU-23             [1, 6144]               0               0\n",
      "         Linear-24                [1, 2]          12,290          12,290\n",
      "=========================================================================\n",
      "Total params: 12,713,934\n",
      "Trainable params: 12,713,934\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create and show model summary\n",
    "model = create_model()\n",
    "from pytorch_model_summary import summary\n",
    "print(summary(model, torch.zeros((1, 1, 24, 256)), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed, model_name, feature, loader_train, loader_val, num_epoch):\n",
    "    '''\n",
    "    Train a model for num_epoch given a random seed. \n",
    "    During training, logs and intemediary models will be saved in files accordingly to model_name\n",
    "    param:\n",
    "        seed: Int - Random seed number\n",
    "        model_name: String - Name of the model to be saved. Used for logging\n",
    "        feature: String - Whether 'raw' or 'topo'\n",
    "        loader_train: DataLoader with training set\n",
    "        loader_val: DataLoader with validation set\n",
    "        num_epoch: Int - number of epoch to train the model\n",
    "    '''\n",
    "    model = create_model()\n",
    "    logger.set_model_save_location(f'{model_name}-{feature}')\n",
    "    experiment = f'{model_name}-{feature}-seed{seed}'\n",
    "    logger.set_experiment(experiment)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # toggle between learning rate and batch size values \n",
    "\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=0.002)\n",
    "    model = train(model, loader_train, loader_val, optimizer, epochs=num_epoch)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSqENpKfB2UE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model multiple times, each with different random seed\n",
    "logger = Logger() # initialize logger to be used throughout\n",
    "\n",
    "batch_size = 256\n",
    "loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(val_data, batch_size=batch_size)\n",
    "for s in range(1):\n",
    "    model = run_experiment(s, 'original-ages-binary-batchNorm', 'raw',loader_train, loader_val,50)\n",
    "# model = run_experiment(9, 'original', 'raw',70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(loader, model):\n",
    "    '''\n",
    "    Check accuracy and get confusion matrix of the model \n",
    "    param:\n",
    "        loader: An EEGDataset object\n",
    "        model: A PyTorch Module to test\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        logger.log('Checking accuracy on training set')\n",
    "    elif loader.dataset.val:\n",
    "        logger.log('Checking accuracy on validation set')\n",
    "    else:\n",
    "        logger.log('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    lossTotal = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             x = x.squeeze(1)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y, weight=torch.tensor([class_0_weight,class_1_weight], dtype=dtype,device=device))\n",
    "            lossTotal += loss.item()\n",
    "#             print(f\"label {y}, prediction {scores}\")\n",
    "            _, preds = scores.max(1)\n",
    "            class_0 = y==0\n",
    "            class_1 = y==1\n",
    "            tp += (preds[class_1] == y[class_1]).sum()\n",
    "            tn += (preds[class_0] == y[class_0]).sum()\n",
    "            fp += (y[class_0] != preds[class_0]).sum()\n",
    "            fn += (y[class_1] != preds[class_1]).sum()\n",
    "            num_correct += (preds == y).sum()\n",
    "            assert num_correct == (tp+tn)\n",
    "            num_samples += preds.size(0)\n",
    "    precision = tp/(tp+fp)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    lossTotal = lossTotal / len(loader)\n",
    "    logger.log('Loss (%f)' % (loss))\n",
    "    logger.log('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    logger.log(f'tp: {tp}')\n",
    "    logger.log(f'tn: {tn}')\n",
    "    logger.log(f'fp: {fp}')\n",
    "    logger.log(f'fn: {fn}')\n",
    "    logger.log(f'precision: {tp/(tp+fp)}')\n",
    "    logger.log(f'recall: {tp/(tp+fn)}')\n",
    "    \n",
    "    return acc,lossTotal,tp,tn,fp,fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, subj_csv):\n",
    "    '''\n",
    "    Test model using two different metrics. First is per sample accuracy.\n",
    "    Second is to use 40 samples per subject and perform voting:\n",
    "        If mean prediction > 0.5, classify as female (1)\n",
    "        Otherwise, classify as male (0)\n",
    "    param:\n",
    "        model: A trained PyTorch Module\n",
    "        test_data: test dataset\n",
    "        subj_csv: spreadsheet containing subject IDs for the test dataset\n",
    "    '''\n",
    "    # one-segment test\n",
    "    logger.log('Testing model accuracy using 1-segment metric')\n",
    "    loader_test = DataLoader(test_data, batch_size=70)\n",
    "    per_sample_acc = confusion_matrix(loader_test, model)\n",
    "\n",
    "    # 40-segment test\n",
    "    logger.log('Testing model accuracy using 40-segment per subject metric')\n",
    "    with open(subj_csv, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        subjIDs = [row[0] for row in spamreader]\n",
    "    unique_subjs,indices = np.unique(subjIDs,return_index=True)\n",
    "\n",
    "    iterable_test_data = list(iter(DataLoader(test_data, batch_size=1)))\n",
    "    num_correct = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for subj,idx in zip(unique_subjs,indices):\n",
    "    #     print(f'Subj {subj} - gender {iterable_test_data[idx][1]}')\n",
    "        data = iterable_test_data[idx:idx+40]\n",
    "        #print(np.sum([y for _,y in data]))\n",
    "        assert 40 == np.sum([y for _,y in data]) or 0 == np.sum([y for _,y in data])\n",
    "        preds = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in data:\n",
    "                x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "                correct = y\n",
    "                scores = model(x)\n",
    "                _, pred = scores.max(1)\n",
    "                preds.append(pred)\n",
    "        final_pred = (torch.mean(torch.FloatTensor(preds)) > 0.5).sum()\n",
    "        if final_pred == 1 and correct == 1:\n",
    "            tp += 1\n",
    "        elif final_pred == 0 and correct == 0:\n",
    "            tn += 1\n",
    "        elif final_pred == 1 and correct == 0:\n",
    "            fp += 1\n",
    "        elif final_pred == 0 and correct == 1:\n",
    "            fn += 1\n",
    "        num_correct.append((final_pred == correct).sum())\n",
    "        assert np.sum(num_correct) == (tp+tn)\n",
    "    #print(len(num_correct))\n",
    "    acc = float(np.sum(num_correct)) / len(unique_subjs)\n",
    "    logger.log('Got %d / %d correct (%.2f)' % (np.sum(num_correct), len(unique_subjs), 100 * acc))\n",
    "    logger.log(f'tp: {tp}')\n",
    "    logger.log(f'tn: {tn}')\n",
    "    logger.log(f'fp: {fp}')\n",
    "    logger.log(f'fn: {fn}')\n",
    "    logger.log(f'precision: {tp/(tp+fp)}')\n",
    "    logger.log(f'recall: {tp/(tp+fn)}')\n",
    "    return per_sample_acc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: 15925, (1, 24, 256)\n",
      "Y_test shape: 15925, ()\n"
     ]
    }
   ],
   "source": [
    "test_data_x, test_data_labels = load_data(path, 'test', winLength, numChan, srate, feature, one_channel)\n",
    "test_data_y = test_data_labels.copy()\n",
    "test_data_y[np.logical_and(test_data_labels <= 2,test_data_labels >= 0)] = 0\n",
    "test_data_y[test_data_labels > 2] = 1\n",
    "test_data = EEGDataset(test_data_x, test_data_y, False, False)\n",
    "print(f'X_test shape: {len(test_data)}, {test_data[0][0].shape}')\n",
    "print(f'Y_test shape: {len(test_data)}, {test_data[0][1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model accuracy using 1-segment metric\n",
      "Checking accuracy on test set\n",
      "Loss (0.528255)\n",
      "Got 11844 / 15925 correct (74.37)\n",
      "tp: 4541\n",
      "tn: 7303\n",
      "fp: 1494\n",
      "fn: 2587\n",
      "precision: 0.7524440884590149\n",
      "recall: 0.6370651125907898\n",
      "Testing model accuracy using 40-segment per subject metric\n",
      "Got 159 / 197 correct (80.71)\n",
      "tp: 61\n",
      "tn: 98\n",
      "fp: 11\n",
      "fn: 27\n",
      "precision: 0.8472222222222222\n",
      "recall: 0.6931818181818182\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(mode='debug')\n",
    "loader_test = DataLoader(test_data, batch_size=1)\n",
    "model.to(device=device)\n",
    "subjIDs_file = 'data/test_subjIDs_fewer_subjects.csv'\n",
    "sam_acc, sub_acc = test_model(model, test_data, subjIDs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SexPrediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
